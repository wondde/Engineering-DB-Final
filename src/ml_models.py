# -*- coding: utf-8 -*-
"""
ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ë¶„ì„ ëª¨ë“ˆ

[ì—­í• ]
- DBì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ì™€ ë¨¸ì‹ ëŸ¬ë‹(ML) ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•¨.
- ì£¼ìš” ë¶„ì„ ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ìŒ.
  1. ì‹¤ì—…ë¥  ì˜ˆì¸¡: ê³¼ê±° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¯¸ë˜ì˜ ì‹¤ì—…ë¥ ì„ ì˜ˆì¸¡í•¨. (ì§€ë„í•™ìŠµ - íšŒê·€)
  2. ì§€ì—­ í´ëŸ¬ìŠ¤í„°ë§: ê³ ìš© íŠ¹ì„±ì´ ë¹„ìŠ·í•œ ì§€ì—­ë¼ë¦¬ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ìŒ. (ë¹„ì§€ë„í•™ìŠµ - êµ°ì§‘í™”)
  3. ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„: ì‹œê°„ì— ë”°ë¥¸ ë°ì´í„° ë³€í™”ì˜ íŒ¨í„´ì„ ë¶„ì„í•¨.

[ì‚¬ìš© í”¼ì²˜(Feature)]
- Feature: ëª¨ë¸ì´ ì˜ˆì¸¡ì„ ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ì…ë ¥ ë³€ìˆ˜.
- ê¸°ì¡´ í”¼ì²˜ 6ê°œ + ì‹ ê·œ í”¼ì²˜ 4ê°œ = ì´ 10ê°œì˜ í”¼ì²˜ë¥¼ ì‚¬ìš©í•¨.
  - ê¸°ì¡´: ì´ì¸êµ¬, ê²½ì œí™œë™ì°¸ê°€ìœ¨, ê³ ìš©ë¥ , ì—°ë„, ì›”, ì§€ì—­ID
  - ì‹ ê·œ: ê³ ìš©ë³´í—˜ê°€ì…ë¥ , ì²­ë…„ê³ ìš©ë¹„ìœ¨, ëŒ€ì¡¸ì·¨ì—…ìë¹„ìœ¨, ì´ì§ë¥ 

[ì¶œë ¥]
- 'output/ml_results/' í´ë”ì— ë¶„ì„ ê²°ê³¼ ê·¸ë˜í”„(PNG íŒŒì¼)ë¥¼ ì €ì¥í•¨.
"""

# --- ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
import logging
from pathlib import Path
from typing import Dict

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor # ì‹¤ì—…ë¥  ì˜ˆì¸¡ ëª¨ë¸
from sklearn.model_selection import train_test_split, cross_val_score # ë°ì´í„° ë¶„í•  ë° êµì°¨ê²€ì¦
from sklearn.preprocessing import StandardScaler # ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (í´ëŸ¬ìŠ¤í„°ë§ìš©)
from sklearn.cluster import KMeans # ì§€ì—­ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸
from sklearn.metrics import mean_squared_error, r2_score, silhouette_score # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ
from sqlalchemy.engine import Engine
from sqlalchemy import text

# --- ë¡œê¹… ë° ê²½ë¡œ/í°íŠ¸ ì„¤ì • ---
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

PROJECT_ROOT = Path(__file__).resolve().parents[1]
OUTPUT_DIR = PROJECT_ROOT / "output" / "ml_results"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Matplotlib ê·¸ë˜í”„ì—ì„œ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ í°íŠ¸ ì„¤ì •
import platform
import warnings
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')

def setup_korean_font():
    """ìš´ì˜ì²´ì œë³„ë¡œ í•œê¸€ í°íŠ¸ë¥¼ ìë™ ì„¤ì •"""
    system = platform.system()

    if system == "Darwin":  # macOS
        plt.rcParams["font.family"] = "AppleGothic"
    elif system == "Windows":  # Windows
        # Windowsì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í°íŠ¸ ì‹œë„
        try:
            plt.rcParams["font.family"] = "Malgun Gothic"
        except:
            plt.rcParams["font.family"] = "sans-serif"
    else:  # Linux
        # Linuxì—ì„œëŠ” NanumGothicì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ DejaVu Sans ì‚¬ìš©
        plt.rcParams["font.family"] = "sans-serif"

    plt.rcParams["axes.unicode_minus"] = False  # ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸ ê¹¨ì§ ë°©ì§€

# í°íŠ¸ ì„¤ì • ì ìš©
setup_korean_font()


def load_ml_dataset(engine: Engine) -> pd.DataFrame:
    """
    ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµì— í•„ìš”í•œ ëª¨ë“  ë°ì´í„°ë¥¼ DBì—ì„œ í†µí•©í•˜ì—¬ ë¶ˆëŸ¬ì˜¤ê³ ,
    ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•  í”¼ì²˜(feature)ë“¤ì„ ê³„ì‚°í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ìƒì„±í•¨.
    """
    # [SQL ì¿¼ë¦¬ ì„¤ëª…]
    # - WITH êµ¬ë¬¸ (CTE): ë³µì¡í•œ ì¿¼ë¦¬ë¥¼ ë…¼ë¦¬ì ì¸ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ì‘ì„±.
    #   - youth_employment: ì—°ë ¹ëŒ€ë³„ ë°ì´í„°ì—ì„œ 'ì²­ë…„(15-29ì„¸)' ì·¨ì—…ì ìˆ˜ë§Œ ë”°ë¡œ ê³„ì‚°.
    #   - education_stats: êµìœ¡ìˆ˜ì¤€ë³„ ë°ì´í„°ì—ì„œ 'ëŒ€ì¡¸ì´ìƒ' ì·¨ì—…ì ìˆ˜ë§Œ ë”°ë¡œ ê³„ì‚°.
    # - JOIN: ì—¬ëŸ¬ í…Œì´ë¸”(ì‹¤ì—…ë¥ , ì¸êµ¬, ê³ ìš©ë³´í—˜ ë“±)ì„ 'region_id'ì™€ 'year_month'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ë‚˜ë¡œ í•©ì¹¨.
    # - íŒŒìƒ ë³€ìˆ˜(í”¼ì²˜) ìƒì„±: ê¸°ì¡´ ë°ì´í„°ë“¤ì„ ì¡°í•©í•˜ì—¬ ìƒˆë¡œìš´ ì˜ë¯¸ë¥¼ ê°–ëŠ” ë³€ìˆ˜ë“¤ì„ ë§Œë“¦.
    #   ì˜ˆ: insurance_coverage_rate (ê³ ìš©ë³´í—˜ê°€ì…ë¥ ) = í”¼ë³´í—˜ì ìˆ˜ / ì „ì²´ ì·¨ì—…ì ìˆ˜
    query = text("""
    WITH youth_employment AS (
        SELECT region_id, year_month,
            SUM(CASE WHEN age_group_id = 11 THEN employed_count ELSE 0 END) as youth_employed,
            SUM(CASE WHEN age_group_id BETWEEN 1 AND 6 THEN employed_count ELSE 0 END) as total_employed_by_age
        FROM fact_employment_by_age GROUP BY region_id, year_month
    ),
    education_stats AS (
        SELECT region_id, year_month,
            SUM(CASE WHEN education_id = 4 THEN employed_count ELSE 0 END) as college_employed,
            SUM(employed_count) as total_employed_by_edu
        FROM fact_employment_by_education GROUP BY region_id, year_month
    )
    SELECT
        u.region_id, r.region_name, u.year_month, u.unemployment_rate,
        p.total_pop, i.insured_count, i.new_insured, i.terminated_insured,
        y.youth_employed, e.college_employed,
        /* --- ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë  íŒŒìƒ í”¼ì²˜(Derived Feature)ë“¤ --- */
        CAST(u.labor_force AS FLOAT) / p.total_pop AS labor_force_ratio,
        CAST(u.employed_persons AS FLOAT) / p.total_pop AS employment_ratio,
        CAST(i.insured_count AS FLOAT) / u.employed_persons AS insurance_coverage_rate,
        CAST(y.youth_employed AS FLOAT) / y.total_employed_by_age AS youth_employment_rate,
        CAST(e.college_employed AS FLOAT) / e.total_employed_by_edu AS college_employment_rate,
        CAST((i.new_insured + i.terminated_insured) AS FLOAT) / i.insured_count AS turnover_rate,
        CAST(SUBSTR(u.year_month, 1, 4) AS INTEGER) AS year,
        CAST(SUBSTR(u.year_month, 6, 2) AS INTEGER) AS month
    FROM fact_unemployment_monthly u
    JOIN dim_region r ON u.region_id = r.region_id
    JOIN fact_population_monthly p ON u.region_id = p.region_id AND u.year_month = p.year_month
    LEFT JOIN fact_employment_insurance i ON u.region_id = i.region_id AND u.year_month = i.year_month
    LEFT JOIN youth_employment y ON u.region_id = y.region_id AND u.year_month = y.year_month
    LEFT JOIN education_stats e ON u.region_id = e.region_id AND u.year_month = e.year_month
    WHERE u.unemployment_rate IS NOT NULL AND p.total_pop > 0
    ORDER BY u.year_month, u.region_id
    """)
    with engine.connect() as conn:
        df = pd.read_sql_query(query, conn)
    logger.info(f"âœ“ ML ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰, {df.shape[1]}ê°œ ì»¬ëŸ¼")
    return df


def train_unemployment_predictor(df: pd.DataFrame) -> Dict:
    """ì‹¤ì—…ë¥  ì˜ˆì¸¡ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í‰ê°€í•¨ (Random Forest, Gradient Boosting)."""
    logger.info("=" * 80)
    logger.info("ğŸ¤– [AI ëª¨ë¸ 1] ì‹¤ì—…ë¥  ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ")
    logger.info("=" * 80)

    # 1. í”¼ì²˜(X)ì™€ íƒ€ê²Ÿ(y) ë³€ìˆ˜ ì„¤ì •
    # - íƒ€ê²Ÿ(y): ìš°ë¦¬ê°€ ì˜ˆì¸¡í•˜ë ¤ëŠ” ê°’. ì—¬ê¸°ì„œëŠ” 'unemployment_rate'(ì‹¤ì—…ë¥ ).
    # - í”¼ì²˜(X): ì˜ˆì¸¡ì— ì‚¬ìš©í•  ì…ë ¥ ê°’ë“¤. ì‹¤ì—…ë¥ ê³¼ ì§ì ‘ì ì¸ ê´€ê³„ê°€ ì—†ëŠ” ë…ë¦½ì ì¸ ë³€ìˆ˜ë“¤ë§Œ ì‚¬ìš©.
    feature_cols = [
        "total_pop", "labor_force_ratio", "employment_ratio", "year", "month", "region_id",
        "insurance_coverage_rate", "youth_employment_rate", "college_employment_rate", "turnover_rate"
    ]
    X = df[feature_cols].copy()
    y = df["unemployment_rate"]

    # 2. ê²°ì¸¡ì¹˜(NaN) ì²˜ë¦¬
    # ML ëª¨ë¸ì€ ë¹ˆ ê°’ì„ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ë¯€ë¡œ, ê²°ì¸¡ì¹˜ê°€ í¬í•¨ëœ í–‰ì€ ì œê±°í•¨.
    # ì˜ëª»ëœ ê°’ìœ¼ë¡œ ì±„ìš°ëŠ”(imputation) ê²ƒë³´ë‹¤, í™•ì‹¤í•œ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ëª¨ë¸ ì„±ëŠ¥ì— ë” ì¢‹ì„ ìˆ˜ ìˆìŒ.
    mask = X.notna().all(axis=1) & y.notna()
    X, y = X[mask], y[mask]
    logger.info(f"ê²°ì¸¡ì¹˜ ì œê±° í›„ í•™ìŠµ ë°ì´í„°: {len(X)}ê±´")

    # 3. í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬ (ì‹œê³„ì—´ ë°ì´í„° ë°©ì‹)
    # [ì¤‘ìš”] ì‹œê³„ì—´ ë°ì´í„°ëŠ” ì ˆëŒ€ ëœë¤ìœ¼ë¡œ ì„ìœ¼ë©´ ì•ˆ ë¨. ê³¼ê±° ë°ì´í„°ë¡œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•˜ê¸° ë•Œë¬¸.
    # ì—¬ê¸°ì„œëŠ” ë°ì´í„°ì˜ ì• 80%ë¥¼ í•™ìŠµìš©ìœ¼ë¡œ, ë’¤ 20%ë¥¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‚¬ìš©.
    # (ì˜ˆ: 2017~2024ë…„ ë°ì´í„°ë¡œ í•™ìŠµ -> 2025ë…„ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸)
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    logger.info(f"í•™ìŠµ ë°ì´í„°: {len(X_train)}ê±´, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê±´")

    # 4. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    # [ëª¨ë¸ 1: ëœë¤ í¬ë ˆìŠ¤íŠ¸]
    # - ìˆ˜ë°± ê°œì˜ ì‘ì€ ì˜ì‚¬ê²°ì •ë‚˜ë¬´(Decision Tree)ë¥¼ ë§Œë“¤ê³ , ê·¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¢…í•©(íˆ¬í‘œ)í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ê°’ì„ ì •í•˜ëŠ” ëª¨ë¸.
    # - ë³‘ë ¬ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•´ í•™ìŠµ ì†ë„ê°€ ë¹ ë¥´ê³ , ì¼ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ì¢‹ìŒ.
    rf_model = RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1)
    rf_model.fit(X_train, y_train)
    rf_pred = rf_model.predict(X_test)
    rf_r2 = r2_score(y_test, rf_pred) # R-squared: ëª¨ë¸ì˜ ì„¤ëª…ë ¥. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ.
    rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred)) # RMSE: ì˜ˆì¸¡ ì˜¤ì°¨. ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ.

    # [ëª¨ë¸ 2: ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…]
    # - ì—¬ëŸ¬ ê°œì˜ ë‚˜ë¬´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë§Œë“¤ë©´ì„œ, ì´ì „ ë‚˜ë¬´ì˜ ì˜ˆì¸¡ ì˜¤ì°¨ë¥¼ ë‹¤ìŒ ë‚˜ë¬´ê°€ ë³´ì™„í•´ë‚˜ê°€ëŠ” ë°©ì‹.
    # - ì¼ë°˜ì ìœ¼ë¡œ ëœë¤ í¬ë ˆìŠ¤íŠ¸ë³´ë‹¤ ì„±ëŠ¥ì´ ì•½ê°„ ë” ì¢‹ì§€ë§Œ, í•™ìŠµ ì†ë„ëŠ” ëŠë¦¼.
    gb_model = GradientBoostingRegressor(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)
    gb_model.fit(X_train, y_train)
    gb_pred = gb_model.predict(X_test)
    gb_r2 = r2_score(y_test, gb_pred)
    gb_rmse = np.sqrt(mean_squared_error(y_test, gb_pred))

    # 5. êµì°¨ ê²€ì¦ (Cross-Validation)
    # - í•™ìŠµ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ê°œ(cv=5)ë¡œ ìª¼ê°œì„œ, ëª¨ë¸ì„ ì—¬ëŸ¬ ë²ˆ í…ŒìŠ¤íŠ¸í•˜ëŠ” ê¸°ë²•.
    # - ëª¨ë¸ì´ íŠ¹ì • ë°ì´í„°ì—ë§Œ ê³¼ë„í•˜ê²Œ ìµœì í™”(ê³¼ì í•©)ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³ , ì¼ë°˜ì ì¸ ì„±ëŠ¥ì„ ì¸¡ì •í•  ìˆ˜ ìˆìŒ.
    rf_cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring="r2")
    gb_cv_scores = cross_val_score(gb_model, X_train, y_train, cv=5, scoring="r2")

    # 6. ê²°ê³¼ ì¶œë ¥ ë° ì‹œê°í™”
    print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
    print(f"{'ëª¨ë¸':<20} {'RÂ² Score':<15} {'RMSE':<15} {'CV RÂ² (í‰ê· )':<15}")
    print("-" * 65)
    print(f"{'Random Forest':<20} {rf_r2:<15.4f} {rf_rmse:<15.4f} {rf_cv_scores.mean():<15.4f}")
    print(f"{'Gradient Boosting':<20} {gb_r2:<15.4f} {gb_rmse:<15.4f} {gb_cv_scores.mean():<15.4f}")

    # í”¼ì²˜ ì¤‘ìš”ë„: ì–´ë–¤ í”¼ì²˜ê°€ ì‹¤ì—…ë¥  ì˜ˆì¸¡ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì³¤ëŠ”ì§€ ë³´ì—¬ì¤Œ.
    feature_importance = pd.DataFrame({
        "feature": feature_cols, "importance": rf_model.feature_importances_
    }).sort_values("importance", ascending=False)
    print("\nğŸ” í”¼ì²˜ ì¤‘ìš”ë„ (ì–´ë–¤ ë³€ìˆ˜ê°€ ì˜ˆì¸¡ì— ê°€ì¥ ì¤‘ìš”í•œê°€?)")
    print(feature_importance.head(5).to_string(index=False))

    # ê·¸ë˜í”„ ì €ì¥
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    axes[0].scatter(y_test, rf_pred, alpha=0.5, s=10)
    axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    axes[0].set_title(f"Random Forest (RÂ²={rf_r2:.4f})")
    axes[1].scatter(y_test, gb_pred, alpha=0.5, s=10, color='green')
    axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    axes[1].set_title(f"Gradient Boosting (RÂ²={gb_r2:.4f})")
    plt.savefig(OUTPUT_DIR / "01_unemployment_prediction.png", dpi=300)
    plt.close()

    fig, ax = plt.subplots(figsize=(10, 6))
    ax.barh(feature_importance["feature"], feature_importance["importance"])
    ax.set_title("ì‹¤ì—…ë¥  ì˜ˆì¸¡ í”¼ì²˜ ì¤‘ìš”ë„")
    plt.savefig(OUTPUT_DIR / "02_feature_importance.png", dpi=300)
    plt.close()

    return {"feature_importance": feature_importance}


def run_region_clustering(df: pd.DataFrame) -> Dict:
    """ì§€ì—­ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ (K-Means)ì„ í†µí•´ ê³ ìš© íŠ¹ì„±ì´ ë¹„ìŠ·í•œ ì§€ì—­ë“¤ì„ ê·¸ë£¹í™”í•¨."""
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ¤– [AI ëª¨ë¸ 2] ì§€ì—­ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ (K-Means)")
    logger.info("=" * 80)

    # 1. í´ëŸ¬ìŠ¤í„°ë§ìš© ë°ì´í„° ìƒì„±
    # ê° ì§€ì—­ë³„ë¡œ ì£¼ìš” ì§€í‘œë“¤ì˜ í‰ê· ê°’ì„ ê³„ì‚°í•¨.
    region_stats = df.groupby("region_name").agg({
        "unemployment_rate": "mean",
        "employment_ratio": "mean",
        "insurance_coverage_rate": "mean",
        "youth_employment_rate": "mean"
    }).dropna()

    # 2. ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (StandardScaler)
    # - K-MeansëŠ” ê±°ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê·¸ë£¹ì„ ë‚˜ëˆ„ê¸° ë•Œë¬¸ì—, ê° í”¼ì²˜ì˜ ë‹¨ìœ„(scale)ê°€ ë‹¤ë¥´ë©´ ì™œê³¡ì´ ë°œìƒí•¨.
    #   (ì˜ˆ: ì¸êµ¬ìˆ˜(ë°±ë§Œ ë‹¨ìœ„)ì™€ ì‹¤ì—…ë¥ (%)ì„ ê·¸ëƒ¥ ë¹„êµí•˜ë©´ ì¸êµ¬ìˆ˜ì˜ ì˜í–¥ì´ í›¨ì”¬ ì»¤ì§)
    # - StandardScalerëŠ” ëª¨ë“  í”¼ì²˜ë¥¼ í‰ê·  0, í‘œì¤€í¸ì°¨ 1ì¸ ì •ê·œë¶„í¬ë¡œ ë³€í™˜í•˜ì—¬ ë‹¨ìœ„ë¥¼ í†µì¼ì‹œì¼œ ì¤Œ.
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(region_stats)

    # 3. ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜(K) ì°¾ê¸°
    # - ì‹¤ë£¨ì—£ ì ìˆ˜(Silhouette Score): -1~1 ì‚¬ì´ì˜ ê°’. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ í´ëŸ¬ìŠ¤í„°ë§ì´ ì˜ ë˜ì—ˆë‹¤ëŠ” ì˜ë¯¸.
    #   (ê°™ì€ í´ëŸ¬ìŠ¤í„° ë‚´ ë°ì´í„°ëŠ” ê°€ê¹ê³ , ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„° ë°ì´í„°ì™€ëŠ” ë©€ë‹¤ëŠ” ëœ»)
    silhouette_scores = []
    K_range = range(2, 10)
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
        kmeans.fit(X_scaled)
        silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))
    
    optimal_k = K_range[np.argmax(silhouette_scores)]
    logger.info(f"ìµœì  í´ëŸ¬ìŠ¤í„° ê°œìˆ˜(K)ëŠ” {optimal_k}ë¡œ ê²°ì •ë¨ (ì‹¤ë£¨ì—£ ì ìˆ˜ ìµœëŒ€).")

    # 4. K-Means ëª¨ë¸ í•™ìŠµ ë° ê²°ê³¼ ë¶„ì„
    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')
    region_stats["cluster"] = kmeans.fit_predict(X_scaled)

    print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ (K={optimal_k})")
    for i in range(optimal_k):
        print(f"\n[í´ëŸ¬ìŠ¤í„° {i}]")
        cluster_regions = region_stats[region_stats["cluster"] == i]
        print(f"  - ì§€ì—­: {', '.join(cluster_regions.index)}")
        print(cluster_regions.describe().loc[["mean", "std"]].round(2).to_string())

    # 5. ì‹œê°í™”
    # PCA: ê³ ì°¨ì›(4ê°œ í”¼ì²˜) ë°ì´í„°ë¥¼ ì‹œê°í™”ë¥¼ ìœ„í•´ 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ëŠ” ê¸°ë²•.
    from sklearn.decomposition import PCA
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)
    
    plt.figure(figsize=(10, 8))
    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=region_stats['cluster'], palette='viridis', s=150, alpha=0.8)
    for i, region in enumerate(region_stats.index):
        plt.text(X_pca[i, 0]+0.05, X_pca[i, 1], region, fontsize=9)
    plt.title(f'ì§€ì—­ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ (K={optimal_k})')
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.grid(True, alpha=0.3)
    plt.savefig(OUTPUT_DIR / "03_region_clustering.png", dpi=300)
    plt.close()

    return {"region_stats": region_stats}


def run_time_series_analysis(df: pd.DataFrame) -> Dict:
    """ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„ì„ í†µí•´ ì „ì²´ ì‹¤ì—…ë¥ ì˜ ì¥ê¸° ì¶”ì„¸ì™€ ê³„ì ˆì„±ì„ íŒŒì•…í•¨."""
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“ˆ [ê¸°ìˆ í†µê³„] ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„")
    logger.info("=" * 80)

    # 1. ì „êµ­ ì›”ë³„ í‰ê·  ì‹¤ì—…ë¥  ê³„ì‚°
    ts_data = df.groupby("year_month")["unemployment_rate"].mean().reset_index()
    ts_data["year_month"] = pd.to_datetime(ts_data["year_month"])
    ts_data = ts_data.set_index("year_month")

    # 2. ì‹œê³„ì—´ ë¶„í•´ (Seasonal Decompose)
    # - ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì¶”ì„¸(Trend), ê³„ì ˆì„±(Seasonality), ì”ì°¨(Residual) ì„¸ ê°€ì§€ ìš”ì†Œë¡œ ë¶„í•´í•¨.
    from statsmodels.tsa.seasonal import seasonal_decompose
    decomposition = seasonal_decompose(ts_data['unemployment_rate'], model='additive', period=12)

    # 3. ì‹œê°í™”
    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 10), sharex=True)
    decomposition.observed.plot(ax=ax1, legend=False)
    ax1.set_ylabel('Observed')
    decomposition.trend.plot(ax=ax2, legend=False)
    ax2.set_ylabel('Trend')
    decomposition.seasonal.plot(ax=ax3, legend=False)
    ax3.set_ylabel('Seasonal')
    decomposition.resid.plot(ax=ax4, legend=False)
    ax4.set_ylabel('Residual')
    plt.suptitle('ì‹¤ì—…ë¥  ì‹œê³„ì—´ ë¶„í•´ (ì „êµ­ í‰ê· )')
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(OUTPUT_DIR / "04_time_series_trend.png", dpi=300)
    plt.close()

    print("ì‹œê³„ì—´ ë¶„í•´ ê²°ê³¼ '04_time_series_trend.png' íŒŒì¼ë¡œ ì €ì¥ë¨.")
    print("- Trend: ë°ì´í„°ì˜ ì¥ê¸°ì ì¸ ì¶”ì„¸")
    print("- Seasonal: íŠ¹ì • ê¸°ê°„(12ê°œì›”)ë§ˆë‹¤ ë°˜ë³µë˜ëŠ” íŒ¨í„´")
    print("- Residual: ì¶”ì„¸ì™€ ê³„ì ˆì„±ìœ¼ë¡œ ì„¤ëª…ë˜ì§€ ì•ŠëŠ” ë‚˜ë¨¸ì§€ ë³€ë™(ë…¸ì´ì¦ˆ)")

    return {"decomposition": decomposition}


def run_all_ml_models(engine: Engine) -> Dict:
    """ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ë¶„ì„ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜."""
    logger.info("\n" + "=" * 80)
    logger.info("ğŸš€ AI/ML ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    logger.info("=" * 80)

    # 1. ML í•™ìŠµìš© ë°ì´í„°ì…‹ ë¡œë“œ
    df = load_ml_dataset(engine)
    results = {}

    # 2. ì‹¤ì—…ë¥  ì˜ˆì¸¡ ëª¨ë¸ ì‹¤í–‰
    results["prediction"] = train_unemployment_predictor(df)

    # 3. ì§€ì—­ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
    results["clustering"] = run_region_clustering(df)

    # 4. ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰
    results["time_series"] = run_time_series_analysis(df)

    logger.info("\n" + "=" * 80)
    logger.info("âœ… AI/ML ë¶„ì„ ì™„ë£Œ!")
    logger.info(f"ğŸ“ ê²°ê³¼ ê·¸ë˜í”„ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")
    logger.info("=" * 80)

    return results


# ì´ íŒŒì¼ì´ ì§ì ‘ ì‹¤í–‰ë  ë•Œ (ì˜ˆ: python src/ml_models.py) ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•¨.
# ì´ ëª¨ë“ˆë§Œ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ìš©ë„ì„.
if __name__ == "__main__":
    from db_loader import DBConfig

    config = DBConfig()
    engine = config.make_engine()

    results = run_all_ml_models(engine)
